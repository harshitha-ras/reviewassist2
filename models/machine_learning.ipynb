{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10942, 2)\n",
      "                                             content sentiment\n",
      "0  I love this app, but I do have one major gripe...  negative\n",
      "1  Trash. Yes, it has some nice nifty features bu...  negative\n",
      "2  OMG the UI is awful, seriously you have popup ...  negative\n",
      "3  I've been using the app for a while and since ...  negative\n",
      "4  Unable to register with an email. Clicking\"con...  negative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_csv(os.path.join('data','sentiment-analysis-dataset-google-play-app-reviews.csv'))\n",
    "df = df[['content','score']] # select content and score\n",
    "df.dropna()\n",
    "df['sentiment'] = df['score'].apply(lambda x: 'positive' if x >= 4 else 'negative' if x <= 2 else 'neutral')\n",
    "df = df[['content','sentiment']]\n",
    "df = df[df['sentiment'] != 'neutral']  # Exclude neutral reviews\n",
    "\n",
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Harry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Harry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Harry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Harry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords','punkt_tab'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_content(content):\n",
    "    '''this function will clear the text by following steps:'''\n",
    "    # step 1: expand contractions \n",
    "    content = contractions.fix(content) \n",
    "    # step 2: convert text to lower \n",
    "    content = content.lower()\n",
    "    # step 3: remove special characters\n",
    "    content = re.sub(r'[^a-zA-Z\\s]', '', content) \n",
    "    # step 4: tokenization\n",
    "    tokens = word_tokenize(content)\n",
    "    # step 5: lemmatization\n",
    "    cleared = []\n",
    "    for word in tokens:\n",
    "        if (word not in stop_words) and len(word) > 2: # exclude stop words and small words like a, an, it, as\n",
    "            cleared.append(lemmatizer.lemmatize(word))\n",
    "    \n",
    "    return ' '.join(cleared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                                                  content sentiment\n",
       "0      love app one major gripe want option buy premi...  negative\n",
       "1      trash yes nice nifty feature lack complete nec...  negative\n",
       "2      omg awful seriously popup premium every second...  negative\n",
       "3      using app since last week acting weird receive...  negative\n",
       "4      unable register email clickingcontinue email t...  negative\n",
       "...                                                  ...       ...\n",
       "16087  used several year one best digital planner fan...  positive\n",
       "16088  love love keep day forever cross like piece pa...  positive\n",
       "16089                                          great app  positive\n",
       "16090                          helpful user friendly app  positive\n",
       "16091  used app year really find helpful like synched...  positive\n",
       "\n",
       "[10942 rows x 2 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'] = df['content'].apply(clear_content)\n",
    "df.head()\n",
    "df.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,1), stop_words='english')\n",
    "X = tfidf.fit_transform(df['content'])\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.90      0.87      1038\n",
      "    positive       0.91      0.85      0.88      1151\n",
      "\n",
      "    accuracy                           0.88      2189\n",
      "   macro avg       0.88      0.88      0.88      2189\n",
      "weighted avg       0.88      0.88      0.88      2189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
    "\n",
    "# logistic regression\n",
    "lr_model = LogisticRegression(max_iter=5000, class_weight='balanced', random_state=0)\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred = lr_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           feature  coefficient  normalized_coefficient\n",
      "2380          love     5.549449               11.126464\n",
      "1773         great     5.390792               10.808801\n",
      "1313          easy     4.608200                9.241892\n",
      "503           best     4.324643                8.674153\n",
      "4905          wish     3.719080                7.461692\n",
      "...            ...          ...                     ...\n",
      "845    complicated    -2.626188               -5.242830\n",
      "4661  uninstalling    -2.649653               -5.289811\n",
      "4659     uninstall    -2.727287               -5.445251\n",
      "1113       deleted    -2.946235               -5.883629\n",
      "4744       useless    -4.585184               -9.165141\n",
      "\n",
      "[5000 rows x 3 columns]\n",
      "\n",
      "Top Positive Keywords:\n",
      "{'love': 11.12646411915547, 'great': 10.808800587495753, 'easy': 9.241891519846098, 'best': 8.674153363777542, 'wish': 7.461691787196903, 'amazing': 7.1930007348137766, 'excellent': 6.861168179124314, 'helpful': 6.80387851382512, 'perfect': 6.747182868473055, 'thank': 6.726046109314852, 'awesome': 6.252755039917394, 'nice': 6.227420134099516, 'helped': 6.0914035991213105, 'life': 6.033034563987242, 'useful': 5.714812582521611, 'good': 5.285960417363126, 'fun': 5.060264258352992, 'highly': 4.6032369947304925, 'little': 4.574163858696889, 'far': 4.416137218304329, 'track': 4.340416806530082, 'home': 4.24233334004171, 'schedule': 4.16791427245065, 'organized': 4.152989415708801, 'better': 4.1466957769555846, 'goal': 4.144692665610895, 'really': 4.109535495316155, 'cool': 4.086385327176585, 'like': 4.063844552940838, 'future': 3.999448876917471}\n",
      "\n",
      "Top Negative Keywords:\n",
      "{'useless': -9.165141411504024, 'deleted': -5.883629036360792, 'uninstall': -5.445250827963048, 'uninstalling': -5.289811328317221, 'complicated': -5.242830318532222, 'waste': -4.996526761437147, 'worst': -4.963565104869967, 'confusing': -4.925995636778115, 'uninstalled': -4.833703864935744, 'crashing': -4.765257494236115, 'longer': -4.725062309846912, 'disappointed': -4.6951880559874875, 'pay': -4.478748642693655, 'sign': -4.318571740866447, 'sync': -4.236788730441583, 'email': -4.235869939098414, 'open': -4.107437525793545, 'ad': -4.077133129709219, 'disappointing': -3.981212241255427, 'subscription': -3.8366388106834717, 'data': -3.8185689660022804, 'suck': -3.8159865680076024, 'unable': -3.743491131567741, 'frustrating': -3.7126008009034077, 'paying': -3.6771720508106176, 'premium': -3.6280799757928675, 'okay': -3.6064384912098877, 'completely': -3.5369867965711297, 'log': -3.5127150888223824, 'idea': -3.471616779040123}\n"
     ]
    }
   ],
   "source": [
    "# feature extractions\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "coef_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': lr_model.coef_[0]\n",
    "    })\n",
    "\n",
    "# normalize the coefficient for display (% of the color)\n",
    "mean_coef = coef_df['coefficient'].mean()\n",
    "std_coef = coef_df['coefficient'].std()\n",
    "coef_df['normalized_coefficient'] = (coef_df['coefficient'] - mean_coef) / std_coef\n",
    "\n",
    "positive_keywords = coef_df.sort_values('coefficient', ascending=False).head(30) # pick top 20\n",
    "negative_keywords = coef_df.sort_values('coefficient').head(30)\n",
    "\n",
    "positive_keywords_dict = positive_keywords.set_index('feature')['normalized_coefficient'].to_dict()\n",
    "negative_keywords_dict = negative_keywords.set_index('feature')['normalized_coefficient'].to_dict()\n",
    "\n",
    "print(coef_df.sort_values('coefficient', ascending=False))\n",
    "print(\"\\nTop Positive Keywords:\")\n",
    "print(positive_keywords_dict)\n",
    "print(\"\\nTop Negative Keywords:\")\n",
    "print(negative_keywords_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other methods for keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "def extract_rake_keywords(texts, top_n=20):\n",
    "    \"\"\"\n",
    "    Extracts 1-gram keywords using the RAKE algorithm.\n",
    "    \"\"\"\n",
    "    r = Rake()\n",
    "    keywords = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # split text into 1-gram\n",
    "        words = text.split()\n",
    "        preprocessed_text = ' '.join(words)\n",
    "\n",
    "        # extract keywords\n",
    "        r.extract_keywords_from_text(preprocessed_text)\n",
    "        for phrase in r.get_ranked_phrases():\n",
    "            if len(phrase.split()) == 1:  # ensure it's a single word\n",
    "                keywords.append(phrase)\n",
    "    \n",
    "    return pd.Series(keywords).value_counts().head(top_n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAKE Positive Keywords:\n",
      "['good', 'great', 'nice', 'love', 'excellent', 'useful', 'awesome', 'amazing', 'helpful', 'perfect', 'best', 'cool', 'like', 'bad', 'super', 'superb', 'fun', 'wonderful', 'loved', 'obsessed']\n",
      "\n",
      "RAKE Negative Keywords:\n",
      "['good', 'complicated', 'bad', 'confusing', 'worst', 'nice', 'working', 'hate', 'terrible', 'expensive', 'useless', 'suck', 'crashing', 'complex', 'okay', 'intuitive', 'reminder', 'sure', 'usefull', 'acceptable']\n"
     ]
    }
   ],
   "source": [
    "# Extract keywords using RAKE for positive and negative reviews separately\n",
    "positive_rake_keywords = extract_rake_keywords(df[df['sentiment'] == 'positive']['content']).index.tolist()\n",
    "negative_rake_keywords = extract_rake_keywords(df[df['sentiment'] == 'negative']['content']).index.tolist()\n",
    "\n",
    "print(\"\\nRAKE Positive Keywords:\")\n",
    "print(positive_rake_keywords)\n",
    "print(\"\\nRAKE Negative Keywords:\")\n",
    "print(negative_rake_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keybert import KeyBERT # not performing well\n",
    "\n",
    "# def extract_keybert_keywords(texts, top_n=20):\n",
    "#     \"\"\"Extracts keywords using KeyBERT.\"\"\"\n",
    "#     combined_text = ' '.join(texts)\n",
    "#     kw_model = KeyBERT()\n",
    "#     keywords = kw_model.extract_keywords(\n",
    "#         combined_text,\n",
    "#         keyphrase_ngram_range=(1,2),\n",
    "#         stop_words='english',\n",
    "#         top_n=top_n\n",
    "#     )\n",
    "#     return dict(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract keywords using KeyBERT for positive and negative reviews separately\n",
    "# positive_keybert_keywords = extract_keybert_keywords(df[df['sentiment'] == 'positive']['content'])\n",
    "# negative_keybert_keywords = extract_keybert_keywords(df[df['sentiment'] == 'negative']['content'])\n",
    "\n",
    "# print(\"\\nKeyBERT Positive Keywords:\")\n",
    "# print(list(positive_keybert_keywords.keys()))\n",
    "# print(\"\\nKeyBERT Negative Keywords:\")\n",
    "# print(list(negative_keybert_keywords.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display - Linear Regression Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pretrained model to models\\trained_model from https://figshare.com/articles/dataset/GoogleNews-vectors-negative300/23601195?file=41403483\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "\n",
    "# Load the model in binary format\n",
    "model_path = os.path.join('trained_model','GoogleNews-vectors-negative300.bin.gz')\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.47624388337135315\n"
     ]
    }
   ],
   "source": [
    "# demo\n",
    "similarity = word2vec_model.similarity('crashing', 'crashes')\n",
    "print(f\"Cosine similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def score_to_trans(x):\n",
    "    '''transformation function (2sigmoid(x)-1) that maps values to number between +1 & -1'''\n",
    "    return 2*1/(1+np.exp(-x)) -1 \n",
    "\n",
    "def calculate_score_lr(new_input: str, positive_words: dict, negative_words: dict, similarity_threshold=0.5):\n",
    "    '''Assign the adjusted weight of each fragment in new input'''\n",
    "    result = {}\n",
    "    new_review_cleared = clear_content(new_input)\n",
    "    new_review_list = new_review_cleared.split(\" \")\n",
    "    agg_words = positive_words | negative_words\n",
    "    for frag in new_review_list:\n",
    "        acc_score = 0\n",
    "        matches = 0\n",
    "        for k, v in agg_words.items():\n",
    "            try:\n",
    "                similarity = word2vec_model.similarity(frag, k)\n",
    "                if similarity >= similarity_threshold:\n",
    "                    acc_score += similarity * v # generate new score for color\n",
    "                    matches += 1\n",
    "            except:\n",
    "                pass # skip the fragments not in the \n",
    "\n",
    "        result[frag] = 0 if matches == 0 else acc_score/matches\n",
    "    '''Generate the transparency based on normal cdf'''\n",
    "    for w in result:\n",
    "        if result[w] != 0: # faster processing\n",
    "            result[w] = score_to_trans(result[w])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['using', 'app', 'week', 'absolutely', 'terrible', 'constant', 'crash', 'poor', 'performance', 'make', 'unusable']\n"
     ]
    }
   ],
   "source": [
    "new_review = \"I've been using this app for weeks and it's absolutely terrible! Constant crashes and poor performance make it unusable.\"\n",
    "# new_review = \"Creating an account is supposed to ALLOW you, not FORCE you. Uninstalled without trying it\"\n",
    "# new_review = \"This last update has brought all kinds of bugs. Lists disappearing randomly, lists getting renamed, permissions randomly getting changed. All of these problems indicated serious bugs in the data model and potentially spillage of customer data. I would suggest looking into a new tasks app because the drop in quality has been dramatic.\"\n",
    "# preprocess\n",
    "new_review_cleared = clear_content(new_review)\n",
    "new_review_list = new_review_cleared.split(\" \")\n",
    "# new_review_list = new_review.split(\" \")\n",
    "print(new_review_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'using': 0, 'app': 0, 'week': 0, 'absolutely': 0.09908282164706539, 'terrible': 0.765633482377807, 'constant': 0, 'crash': -0.8646707037832555, 'poor': 0, 'performance': 0, 'make': 0, 'unusable': -0.991033953997229}\n"
     ]
    }
   ],
   "source": [
    "scores = calculate_score_lr(new_review, positive_keywords_dict, negative_keywords_dict, similarity_threshold=0.5)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_sentence_html(sentence:str, sentiment_dict:dict, similarity_threshold=0.5):\n",
    "    words = sentence.split()\n",
    "    highlighted_words = [] # for display\n",
    "    \n",
    "    for word in words:\n",
    "        clean_word = word.lower().strip('.,!?;:()[]{}\"\"\\'')\n",
    "\n",
    "        # assign score to the raw text\n",
    "        for k in sentiment_dict:\n",
    "            try: # secondary mapping\n",
    "                similarity = word2vec_model.similarity(clean_word, k)\n",
    "                if similarity >= similarity_threshold:\n",
    "                    value = similarity*sentiment_dict[k]      \n",
    "                    break # break when find the first match\n",
    "                else: # match below treshold\n",
    "                    value = 0\n",
    "            except: # no match\n",
    "                value = 0\n",
    "        \n",
    "        # formatted output\n",
    "        if value < 0:\n",
    "            # negative word -> red\n",
    "            brightness = 255 - int((abs(value) * 255))\n",
    "            color = f\"rgb(255, {brightness}, {brightness})\"\n",
    "            highlighted_words.append(f'<span style=\"background-color: {color};\">{word}</span>')\n",
    "        elif value > 0:\n",
    "            # positive words -> green\n",
    "            brightness = 255 - int((value * 255))\n",
    "            color = f\"rgb({brightness}, 255, {brightness})\"\n",
    "            highlighted_words.append(f'<span style=\"background-color: {color};\">{word}</span>')\n",
    "        else: # below treshold or no match, directly append\n",
    "            highlighted_words.append(word)\n",
    "    \n",
    "    return ' '.join(highlighted_words)\n",
    "\n",
    "highlighted_html = highlight_sentence_html(new_review, scores)\n",
    "\n",
    "# save to an HTML \n",
    "with open(\"highlighted_sentence.html\", \"w\") as file:\n",
    "    file.write(f\"<html><body style='font-size: 18px; padding: 20px;'>{highlighted_html}</body></html>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display - RAKE Score\n",
    "Idea: calulate the average similarity to positive and negative to assign scores to each words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_center_similarity(word:str, word_list:list):\n",
    "    '''this function calculates the similarity between a word to the center point of a list of words'''\n",
    "    # find center point\n",
    "    vectors = []\n",
    "    for w in word_list:\n",
    "        vectors.append(word2vec_model[w])\n",
    "    centroid_vector = np.mean(vectors, axis=0) \n",
    "    word_vecotor = word2vec_model[word]\n",
    "    # similarity\n",
    "    similarity = np.dot(centroid_vector, word_vecotor) / (np.linalg.norm(centroid_vector) * np.linalg.norm(word_vecotor))\n",
    "\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def calculate_score_rake(new_input: str, positive_words: list, negative_words: list, treshold = 0.1):\n",
    "    '''This function calculates the average score for each keywords'''\n",
    "    result = {}\n",
    "    new_review_cleared = clear_content(new_input)\n",
    "    new_review_list = new_review_cleared.split(\" \")\n",
    "\n",
    "    for frag in new_review_list: # compare similarity of the word to center of the list\n",
    "        if frag not in word2vec_model:\n",
    "            result[frag] = 0\n",
    "        else:\n",
    "            pos_similarity = word_to_center_similarity(frag, positive_words)\n",
    "            neg_similarity = word_to_center_similarity(frag, negative_words)\n",
    "            score = pos_similarity-neg_similarity\n",
    "            if score > treshold:\n",
    "                result[frag] = 1\n",
    "            elif score < -treshold:\n",
    "                result[frag] = -1\n",
    "            else:\n",
    "                result[frag] = 0\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'using': 0, 'app': 0, 'week': 0, 'absolutely': 0, 'terrible': 0, 'constant': 0, 'crash': -1, 'poor': 0, 'performance': 0, 'make': 0, 'unusable': -1}\n"
     ]
    }
   ],
   "source": [
    "scores_rake = calculate_score_rake(new_review, positive_rake_keywords, negative_rake_keywords)\n",
    "print(scores_rake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlighted_html = highlight_sentence_html(new_review, scores_rake)\n",
    "with open(\"highlighted_sentence_rake.html\", \"w\") as file:\n",
    "    file.write(f\"<html><body style='font-size: 18px; padding: 20px;'>{highlighted_html}</body></html>\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
