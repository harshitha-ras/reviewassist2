# -*- coding: utf-8 -*-
"""machine_learning (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r6sO35gPzYwok2zLYYqxJGE7pNBMGore

## Import data from the csv file
"""

import pandas as pd
import os

df = pd.read_csv(os.path.join('data','sentiment-analysis-dataset-google-play-app-reviews.csv'))
df = df[['content','score']] # select content and score
df.dropna()
df['sentiment'] = df['score'].apply(lambda x: 'positive' if x >= 4 else 'negative' if x <= 2 else 'neutral')
df = df[['content','sentiment']]
df = df[df['sentiment'] != 'neutral']  # Exclude neutral reviews

print(df.shape)
print(df.head())

"""## Clear content"""

import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import contractions

nltk.download(['punkt', 'wordnet', 'stopwords','punkt_tab'])
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def clear_content(content):
    '''this function will clear the text by following steps:'''
    # step 1: expand contractions
    content = contractions.fix(content)
    # step 2: convert text to lower
    content = content.lower()
    # step 3: remove special characters
    content = re.sub(r'[^a-zA-Z\s]', '', content)
    # step 4: tokenization
    tokens = word_tokenize(content)
    # step 5: lemmatization
    cleared = []
    for word in tokens:
        if (word not in stop_words) and len(word) > 2: # exclude stop words and small words like a, an, it, as
            cleared.append(lemmatizer.lemmatize(word))

    return ' '.join(cleared)

df['content'] = df['content'].apply(clear_content)
df.head()
df.describe

"""## Vectorization"""

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,1), stop_words='english')
X = tfidf.fit_transform(df['content'])
y = df['sentiment']

"""## Train"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)

# logistic regression
lr_model = LogisticRegression(max_iter=5000, class_weight='balanced', random_state=0)
lr_model.fit(X_train, y_train)
y_pred = lr_model.predict(X_test)
print(classification_report(y_test, y_pred))

"""## Feature Extraction"""

# feature extractions
feature_names = tfidf.get_feature_names_out()
coef_df = pd.DataFrame({
        'feature': feature_names,
        'coefficient': lr_model.coef_[0]
    })

# normalize the coefficient for display (% of the color)
mean_coef = coef_df['coefficient'].mean()
std_coef = coef_df['coefficient'].std()
coef_df['normalized_coefficient'] = (coef_df['coefficient'] - mean_coef) / std_coef

positive_keywords = coef_df.sort_values('coefficient', ascending=False).head(30) # pick top 20
negative_keywords = coef_df.sort_values('coefficient').head(30)

positive_keywords_dict = positive_keywords.set_index('feature')['normalized_coefficient'].to_dict()
negative_keywords_dict = negative_keywords.set_index('feature')['normalized_coefficient'].to_dict()

print(coef_df.sort_values('coefficient', ascending=False))
print("\nTop Positive Keywords:")
print(positive_keywords_dict)
print("\nTop Negative Keywords:")
print(negative_keywords_dict)

"""## Other methods for keywords"""

from rake_nltk import Rake

def extract_rake_keywords(texts, top_n=20):
    """
    Extracts 1-gram keywords using the RAKE algorithm.
    """
    r = Rake()
    keywords = []

    for text in texts:
        # split text into 1-gram
        words = text.split()
        preprocessed_text = ' '.join(words)

        # extract keywords
        r.extract_keywords_from_text(preprocessed_text)
        for phrase in r.get_ranked_phrases():
            if len(phrase.split()) == 1:  # ensure it's a single word
                keywords.append(phrase)

    return pd.Series(keywords).value_counts().head(top_n)

# Extract keywords using RAKE for positive and negative reviews separately
positive_rake_keywords = extract_rake_keywords(df[df['sentiment'] == 'positive']['content']).index.tolist()
negative_rake_keywords = extract_rake_keywords(df[df['sentiment'] == 'negative']['content']).index.tolist()

print("\nRAKE Positive Keywords:")
print(positive_rake_keywords)
print("\nRAKE Negative Keywords:")
print(negative_rake_keywords)

# from keybert import KeyBERT # not performing well

# def extract_keybert_keywords(texts, top_n=20):
#     """Extracts keywords using KeyBERT."""
#     combined_text = ' '.join(texts)
#     kw_model = KeyBERT()
#     keywords = kw_model.extract_keywords(
#         combined_text,
#         keyphrase_ngram_range=(1,2),
#         stop_words='english',
#         top_n=top_n
#     )
#     return dict(keywords)

# # Extract keywords using KeyBERT for positive and negative reviews separately
# positive_keybert_keywords = extract_keybert_keywords(df[df['sentiment'] == 'positive']['content'])
# negative_keybert_keywords = extract_keybert_keywords(df[df['sentiment'] == 'negative']['content'])

# print("\nKeyBERT Positive Keywords:")
# print(list(positive_keybert_keywords.keys()))
# print("\nKeyBERT Negative Keywords:")
# print(list(negative_keybert_keywords.keys()))

"""## Display - Linear Regression Scores"""

# download pretrained model to models\trained_model from https://figshare.com/articles/dataset/GoogleNews-vectors-negative300/23601195?file=41403483
from gensim.models import KeyedVectors
import os

# Load the model in binary format
model_path = os.path.join('trained_model','GoogleNews-vectors-negative300.bin.gz')
word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)

# demo
similarity = word2vec_model.similarity('crashing', 'crashes')
print(f"Cosine similarity: {similarity}")

import numpy as np

def score_to_trans(x):
    '''transformation function (2sigmoid(x)-1) that maps values to number between +1 & -1'''
    return 2*1/(1+np.exp(-x)) -1

def calculate_score_lr(new_input: str, positive_words: dict, negative_words: dict, similarity_threshold=0.5):
    '''Assign the adjusted weight of each fragment in new input'''
    result = {}
    new_review_cleared = clear_content(new_input)
    new_review_list = new_review_cleared.split(" ")
    agg_words = positive_words | negative_words
    for frag in new_review_list:
        acc_score = 0
        matches = 0
        for k, v in agg_words.items():
            try:
                similarity = word2vec_model.similarity(frag, k)
                if similarity >= similarity_threshold:
                    acc_score += similarity * v # generate new score for color
                    matches += 1
            except:
                pass # skip the fragments not in the

        result[frag] = 0 if matches == 0 else acc_score/matches
    '''Generate the transparency based on normal cdf'''
    for w in result:
        if result[w] != 0: # faster processing
            result[w] = score_to_trans(result[w])
    return result

new_review = "I've been using this app for weeks and it's absolutely terrible! Constant crashes and poor performance make it unusable."
# new_review = "Creating an account is supposed to ALLOW you, not FORCE you. Uninstalled without trying it"
# new_review = "This last update has brought all kinds of bugs. Lists disappearing randomly, lists getting renamed, permissions randomly getting changed. All of these problems indicated serious bugs in the data model and potentially spillage of customer data. I would suggest looking into a new tasks app because the drop in quality has been dramatic."
# preprocess
new_review_cleared = clear_content(new_review)
new_review_list = new_review_cleared.split(" ")
# new_review_list = new_review.split(" ")
print(new_review_list)

scores = calculate_score_lr(new_review, positive_keywords_dict, negative_keywords_dict, similarity_threshold=0.5)
print(scores)

def highlight_sentence_html(sentence:str, sentiment_dict:dict, similarity_threshold=0.5):
    words = sentence.split()
    highlighted_words = [] # for display

    for word in words:
        clean_word = word.lower().strip('.,!?;:()[]{}""\'')

        # assign score to the raw text
        for k in sentiment_dict:
            try: # secondary mapping
                similarity = word2vec_model.similarity(clean_word, k)
                if similarity >= similarity_threshold:
                    value = similarity*sentiment_dict[k]
                    break # break when find the first match
                else: # match below treshold
                    value = 0
            except: # no match
                value = 0

        # formatted output
        if value < 0:
            # negative word -> red
            brightness = 255 - int((abs(value) * 255))
            color = f"rgb(255, {brightness}, {brightness})"
            highlighted_words.append(f'<span style="background-color: {color};">{word}</span>')
        elif value > 0:
            # positive words -> green
            brightness = 255 - int((value * 255))
            color = f"rgb({brightness}, 255, {brightness})"
            highlighted_words.append(f'<span style="background-color: {color};">{word}</span>')
        else: # below treshold or no match, directly append
            highlighted_words.append(word)

    return ' '.join(highlighted_words)

highlighted_html = highlight_sentence_html(new_review, scores)

# save to an HTML
with open("highlighted_sentence.html", "w") as file:
    file.write(f"<html><body style='font-size: 18px; padding: 20px;'>{highlighted_html}</body></html>")

"""## Display - RAKE Score
Idea: calulate the average similarity to positive and negative to assign scores to each words.
"""

def word_to_center_similarity(word:str, word_list:list):
    '''this function calculates the similarity between a word to the center point of a list of words'''
    # find center point
    vectors = []
    for w in word_list:
        vectors.append(word2vec_model[w])
    centroid_vector = np.mean(vectors, axis=0)
    word_vecotor = word2vec_model[word]
    # similarity
    similarity = np.dot(centroid_vector, word_vecotor) / (np.linalg.norm(centroid_vector) * np.linalg.norm(word_vecotor))

    return similarity


def calculate_score_rake(new_input: str, positive_words: list, negative_words: list, treshold = 0.1):
    '''This function calculates the average score for each keywords'''
    result = {}
    new_review_cleared = clear_content(new_input)
    new_review_list = new_review_cleared.split(" ")

    for frag in new_review_list: # compare similarity of the word to center of the list
        if frag not in word2vec_model:
            result[frag] = 0
        else:
            pos_similarity = word_to_center_similarity(frag, positive_words)
            neg_similarity = word_to_center_similarity(frag, negative_words)
            score = pos_similarity-neg_similarity
            if score > treshold:
                result[frag] = 1
            elif score < -treshold:
                result[frag] = -1
            else:
                result[frag] = 0

    return result

scores_rake = calculate_score_rake(new_review, positive_rake_keywords, negative_rake_keywords)
print(scores_rake)

highlighted_html = highlight_sentence_html(new_review, scores_rake)
with open("highlighted_sentence_rake.html", "w") as file:
    file.write(f"<html><body style='font-size: 18px; padding: 20px;'>{highlighted_html}</body></html>")